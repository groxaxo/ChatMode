# ChatMode Configuration v3.0
# Copy this file to .env and update with your settings

# ============================================================================
# Provider Auto-Discovery Settings
# ============================================================================
# Set to 'true' to scan shell config files (.bashrc, .zshrc, .bash_profile)
# for API keys on startup. This allows you to keep API keys in your shell
# config instead of the .env file.
SCAN_SHELL_CONFIGS=false

# ============================================================================
# LLM Provider Configuration (Auto-Discovery)
# ============================================================================
# 
# The system now supports automatic provider discovery and model syncing!
# Simply set the API keys below and the system will:
# 1. Auto-detect the provider type from the URL
# 2. Fetch available models from the provider
# 3. Keep models in sync automatically
#
# Supported providers: OpenAI, Ollama, Fireworks AI, DeepSeek, xAI (Grok),
#                      Anthropic, LM Studio, vLLM, and any OpenAI-compatible API
#
# Alternative: You can also add API keys to your ~/.bashrc or ~/.zshrc:
#   export OPENAI_API_KEY=sk-your-key-here
# Then enable SCAN_SHELL_CONFIGS=true above

# === OpenAI (or OpenAI-compatible API) ===
# Get your API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-your-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# === Ollama (Local LLM) ===
# Install: https://ollama.com
# Pull models: ollama pull llama3.2:3b
# The system will auto-sync all your installed Ollama models!
OLLAMA_BASE_URL=http://localhost:11434

# === Fireworks AI (DeepSeek, Llama, etc.) ===
# Get your API key from https://fireworks.ai/account/api-keys
FIREWORKS_API_KEY=fw-your-key-here

# === DeepSeek ===
# Get your API key from https://platform.deepseek.com/api_keys
DEEPSEEK_API_KEY=sk-your-key-here

# === xAI (Grok) ===
# Get your API key from https://console.x.ai
XAI_API_KEY=xai-your-key-here

# === Anthropic (Claude) ===
# Get your API key from https://console.anthropic.com/settings/keys
ANTHROPIC_API_KEY=sk-ant-your-key-here

# === LM Studio (Local) ===
# Download from https://lmstudio.ai
# Models auto-sync when LM Studio is running
LMSTUDIO_BASE_URL=http://localhost:1234/v1

# === vLLM (Self-hosted) ===
# Deploy your own models with vLLM
VLLM_BASE_URL=http://localhost:8000/v1

# === Custom Providers ===
# Add any OpenAI-compatible provider using this format:
# PROVIDER_<NAME>_URL=https://api.example.com/v1
# PROVIDER_<NAME>_KEY=your-api-key
#
# Example:
# PROVIDER_PERPLEXITY_URL=https://api.perplexity.ai/v1
# PROVIDER_PERPLEXITY_KEY=pplx-your-key-here

# ============================================================================
# Embedding Configuration (for semantic memory)
# ============================================================================

# Provider: "ollama" or "openai"
EMBEDDING_PROVIDER=ollama

# === Ollama Embeddings (Recommended for local) ===
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_BASE_URL=http://localhost:11434
EMBEDDING_API_KEY=

# === OpenAI Embeddings (Cloud) ===
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small
# EMBEDDING_BASE_URL=https://api.openai.com/v1
# EMBEDDING_API_KEY=sk-your-key-here

# ============================================================================
# Text-to-Speech (TTS) Configuration
# ============================================================================

# Enable/disable voice synthesis
TTS_ENABLED=false

# === OpenAI TTS ===
TTS_BASE_URL=https://api.openai.com/v1
TTS_API_KEY=sk-your-key-here
TTS_MODEL=tts-1
# Available voices: alloy, echo, fable, onyx, nova, shimmer
TTS_VOICE=alloy

# === Local TTS (Optional) ===
# TTS_BASE_URL=http://localhost:5002/v1
# TTS_API_KEY=not-needed

# Output directory for generated audio files
TTS_OUTPUT_DIR=./tts_out

# ============================================================================
# Storage Configuration
# ============================================================================

# ChromaDB vector database directory
CHROMA_DIR=./data/chroma

# Database URL (SQLite, PostgreSQL, MySQL)
DATABASE_URL=sqlite:///./data/chatmode.db

# For PostgreSQL in production:
# DATABASE_URL=postgresql://user:password@localhost:5432/chatmode

# ============================================================================
# Conversation Settings
# ============================================================================

# Maximum tokens in context window
MAX_CONTEXT_TOKENS=32000

# Maximum tokens per agent response
MAX_OUTPUT_TOKENS=512

# Number of semantic memories to retrieve per query
MEMORY_TOP_K=5

# Maximum recent messages to include in context
HISTORY_MAX_MESSAGES=20

# LLM temperature (0.0=deterministic, 2.0=very creative)
TEMPERATURE=0.9

# Delay between agent responses (seconds)
SLEEP_SECONDS=2

# ============================================================================
# Admin & Debug Settings
# ============================================================================

# Use LLM to auto-generate debate topics
ADMIN_USE_LLM=true

# Optional: Pre-set topic (overrides prompt and LLM generation)
ADMIN_TOPIC=

# ============================================================================
# Logging Configuration
# ============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO)
LOG_LEVEL=INFO

# Enable debug mode with extra verbose output and detailed logs
DEBUG_MODE=false

# Log directory (default: ./logs)
LOG_DIR=./logs

# Enable structured JSON logging (useful for log aggregation systems)
# STRUCTURED_LOGGING=false

# Legacy verbose flag (deprecated, use LOG_LEVEL=DEBUG instead)
VERBOSE=false

# ============================================================================
# Security (Production Only)
# ============================================================================

# Secret key for JWT signing (generate with: openssl rand -hex 32)
SECRET_KEY=change-this-in-production

# CORS allowed origins (comma-separated, use * for development only)
ALLOWED_ORIGINS=*

# For production, set specific origins:
# ALLOWED_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# ============================================================================
# CrewAI Compatibility (Optional)
# ============================================================================

# Ollama embeddings for CrewAI agents
EMBEDDINGS_OLLAMA_MODEL_NAME=nomic-embed-text
EMBEDDINGS_OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# Frontend Configuration (Optional)
# ============================================================================

# Custom frontend directory (if not using default)
# FRONTEND_DIR=/path/to/custom/frontend
